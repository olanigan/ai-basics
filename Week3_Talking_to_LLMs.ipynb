{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP67mVobtxLfjdW073McGEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olanigan/ai-basics/blob/main/Week3_Talking_to_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concepts"
      ],
      "metadata": {
        "id": "j3D8HERDuV_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Abstraction\n",
        "\n",
        "question = \"Why is the sky blue?\"\n",
        "print_llm_response(question)\n",
        "\n",
        "1. Create a prompt based on the question (Dynamic prompting)\n",
        "2. Make a HTTP request to the LLM API\n",
        "\n",
        "\n",
        "Langchain, LlamaIndex\n",
        "CrewAI, Autogen etc. -> Task, Agent (Backstory, Tools)"
      ],
      "metadata": {
        "id": "TOUqM0zyuY5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct HTTP Requests"
      ],
      "metadata": {
        "id": "RZYqp2uEnaeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "lzmfiJX3nhyk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyFTK5D0nNDW",
        "outputId": "dfdbd2f7-1b0e-4e88-c231-04bde105c3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you today? in Yoruba is: \"Bawo, ṣe daadaa ni iwọ loni?\"\n"
          ]
        }
      ],
      "source": [
        "# prompt: make http call to OpenAI API using request\n",
        "\n",
        "!pip install -q requests python-dotenv\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "# When using Google Colab\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "# Set OpenAI key in environment\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "#When using Local environment\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "# .env or .env.local\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "}\n",
        "\n",
        "# Example: Get models\n",
        "def get_models():\n",
        "    url = \"https://api.openai.com/v1/models\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "# Example: Create a completion\n",
        "def create_completion(user_prompt):\n",
        "    url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    data = {\n",
        "        \"model\": \"gpt-4o-mini\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are helpful AI assistant\"},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        \"max_tokens\": 150,\n",
        "        \"n\": 1,\n",
        "        \"stop\": None,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "# Get OpenAI Model list\n",
        "# models = get_models()\n",
        "# print(json.dumps(models, indent=2))\n",
        "\n",
        "user_input = \"Translate this to Yoruba: Hello, how are you today?\"\n",
        "\n",
        "completion = create_completion(user_input)\n",
        "# print(json.dumps(completion, indent=2))\n",
        "print(completion['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini"
      ],
      "metadata": {
        "id": "BroSegx4qC5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "# When using Google Colab\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Set Gemini key in environment\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "def generate_gemini_content(user_input):\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={api_key}\"\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"parts\": [\n",
        "                    {\n",
        "                        \"text\": user_input\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "user_input = \"Translate this to Yoruba: Hello, how are you today?\"\n",
        "\n",
        "# gemini_response = generate_gemini_content(user_input)\n",
        "# print(json.dumps(gemini_response, indent=2))\n",
        "\n",
        "# To extract the generated text (adapt based on the actual response structure):\n",
        "text_response = gemini_response['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "print(text_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bs29MSsqCcm",
        "outputId": "bf062519-c41f-4ee0-c2ff-217ff78abe40"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few ways to translate \"Hello, how are you today?\" into Yoruba, with slight variations in formality and emphasis:\n",
            "\n",
            "**Common Options:**\n",
            "\n",
            "*   **Ẹ n lẹ o, bawo ni ara yin ṣe wa loni?** (This is a good general option, polite and widely understood.)\n",
            "    *   **Ẹ n lẹ o:** Hello (literally, you have arrived well)\n",
            "    *   **bawo ni:** how is\n",
            "    *   **ara yin:** your body/being (formal \"you\")\n",
            "    *   **ṣe wa:** is/doing\n",
            "    *   **loni:** today\n",
            "*   **Bawo ni o ṣe wa loni?** (Less formal, but still polite, suitable for someone you know well.)\n",
            "    *   **bawo ni:** how is\n",
            "    *   **o:** you (informal)\n",
            "    *   **ṣe wa:** is/doing\n",
            "    *   **loni:** today\n",
            "*   **Ṣe alafia ni o wa loni?** (This emphasizes \"are you well?\" instead of just asking \"how are you?\")\n",
            "    *   **Ṣe alafia ni:** Is it well/peace?\n",
            "    *   **o wa:** you are\n",
            "    *   **loni:** today\n",
            "\n",
            "**Less Common but Still Possible:**\n",
            "\n",
            "*   **Ẹ ku ijọkọ, bawo ni ara yin?** (This literally means \"Greetings for sitting\" and is a bit more old-fashioned.)\n",
            "    *   **Ẹ ku ijọkọ:** Greetings for sitting (often used in the morning)\n",
            "    *   **bawo ni ara yin:** how is your body/being (formal)\n",
            "\n",
            "**Choosing the Right One:**\n",
            "\n",
            "*   For someone you don't know well or someone older than you, use **Ẹ n lẹ o, bawo ni ara yin ṣe wa loni?**\n",
            "*   For a friend or someone you know well, **Bawo ni o ṣe wa loni?** is suitable.\n",
            "*   If you want to focus on their well-being, you could ask **Ṣe alafia ni o wa loni?**\n",
            "*   The last one (**Ẹ ku ijọkọ, bawo ni ara yin?**) is less common in modern everyday conversation.\n",
            "\n",
            "So, the most versatile and commonly used translation is:\n",
            "\n",
            "**Ẹ n lẹ o, bawo ni ara yin ṣe wa loni?**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI SDK\n",
        "\n",
        "The OpenAI SDK was released to provide an easier way to call the OpenAI models.\n",
        "\n",
        "However, it has become the de-facto way of talking to LLMs as other inference providers offer OpenAI-compatible endpoints\n"
      ],
      "metadata": {
        "id": "FvBblOuhr6rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## OpenAI LLMs\n",
        "!pip install -q openai\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWn3VWs8r4gf",
        "outputId": "bf28b4fc-d325-48e9-a28f-4e1124c30b8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you today? in Yoruba is: \"Bawo, bawo ni ìwọ ṣe wà loni?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Google Gemini LLMs\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get('GOOGLE_API_KEY'),\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkHul9_hurUJ",
        "outputId": "921bad82-b6ef-4259-aa0a-62f8fae5c4b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are several ways to say \"Hello, how are you today?\" in Yoruba, depending on the level of formality and the time of day. Here are a few options:\n",
            "\n",
            "* **E kaaro, bawo ni o se wa loni?**  This is a common and fairly formal way to greet someone in the afternoon/evening.  \"E kaaro\" is a general afternoon/evening greeting.\n",
            "\n",
            "* **E kaaro, se o daadaa loni?** This is also a common and fairly formal way, using \"se o daadaa\" which directly translates to \"are you well?\".\n",
            "\n",
            "* **E ku ile, bawo ni o se wa loni?** This is appropriate for a morning greeting. \"E ku ile\" means \"good morning\".\n",
            "\n",
            "* **Mo ki e, bawo ni o se wa loni?** This is a more informal greeting, using \"Mo ki e\" which is a more casual greeting.\n",
            "\n",
            "* **Bawo ni o se wa loni?**  You could simply say this, omitting the greeting, particularly if you've already greeted the person.  It's less formal.\n",
            "\n",
            "\n",
            "The best option depends on the context.  For most situations, **E kaaro, bawo ni o se wa loni?** or **E kaaro, se o daadaa loni?** would be suitable.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Outputs"
      ],
      "metadata": {
        "id": "SsyBb2B4zaME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response -> Open-ended or Structured"
      ],
      "metadata": {
        "id": "FkLgCBrz0RKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Model data\n",
        "class CalendarEvent(BaseModel):\n",
        "    name: str\n",
        "    date: str\n",
        "    participants: list[str]\n",
        "\n",
        "completion = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Bola and Ade are going to a Katsina on Friday.\"},\n",
        "    ],\n",
        "    response_format=CalendarEvent,\n",
        ")\n",
        "\n",
        "event = completion.choices[0].message.parsed"
      ],
      "metadata": {
        "id": "hr4pgy8GzdFq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event"
      ],
      "metadata": {
        "id": "VWAtsIs101Vk",
        "outputId": "0622df3a-30fd-47cf-841c-e6217165cfa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CalendarEvent(name='Trip to Katsina', date='Friday', participants=['Bola', 'Ade'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructor"
      ],
      "metadata": {
        "id": "U2d7UNzu1WSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze | grep google-generativeai\n",
        "!pip freeze | grep google-genai"
      ],
      "metadata": {
        "id": "LDdOKId52Hkz",
        "outputId": "e963159e-5e3f-4f85-924f-f97f9ec467f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google-genai==0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q instructor \"instructor[google-generativeai]\" google-genai"
      ],
      "metadata": {
        "id": "EsNTUCxD1QFH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import instructor\n",
        "import google.generativeai as genai\n",
        "\n",
        "api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "client = instructor.from_gemini(\n",
        "    client=genai.GenerativeModel(\n",
        "        model_name=\"models/gemini-2.0-flash-exp\",\n",
        "    ),\n",
        "    mode=instructor.Mode.GEMINI_JSON,\n",
        ")\n",
        "\n",
        "# note that client.chat.completions.create will also work\n",
        "resp = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Bola and Ade are going to a Katsina on Friday.\",\n",
        "        }\n",
        "    ],\n",
        "    response_model=CalendarEvent,\n",
        ")"
      ],
      "metadata": {
        "id": "qILI_7op1ikf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp"
      ],
      "metadata": {
        "id": "FzmlEb4I2tXJ",
        "outputId": "1a24a243-4ed4-4f28-f495-7813d5e76a67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CalendarEvent(name='Katsina Trip', date='Friday', participants=['Bola', 'Ade'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agentic workflows- \"Augmnented LLM\" - Think, Plan, Action, Retrieve\n",
        "\n",
        "\n",
        "What is the weather in Lagos\n",
        "LLM -> idk\n",
        "\n",
        "WeatherAPI (Location)\n",
        "\n",
        "LLM (WeatherAPI -> Lagos)\n",
        "\n",
        "WeatherAPI (Long, Lat)\n",
        "LLM (Lagos -> Long, Lat -> WeatherAPI\n",
        "LLM (Temperature is 30C)\n",
        "Final response = The temp in Lagos is 30C"
      ],
      "metadata": {
        "id": "M7X88Yix24jg"
      }
    }
  ]
}